{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jan 30, 2020\n",
    "# Joined the 5 classification tables available\n",
    "# to be evaluated:\n",
    "    # join is left by default\n",
    "    # because I adopt cit_received as the leftmost data\n",
    "        # this dataset keep data on the patents who received citation\n",
    "    # I can make the first join as an 'outer' operation between cit_received and cit_made\n",
    "    # To be complete, I can begin with patent.csv.gz and make the first two joins with 'outer' type\n",
    "#     Ill git add now and try this\n",
    "    \n",
    "\n",
    "# Jan 29, 2020\n",
    "# many errors appeared after joining the dataset \n",
    "# i rewritten code into simpler scripts\n",
    "# this script joins variables together to be processed later\n",
    "\n",
    "# they are:\n",
    "#     - cit_delay - reads clean_patent and clean_uspatentcitation and calculates delay by patent\n",
    "#     - cit_made - reads clean_uspatentcitation and calculates citations made \n",
    "#     - cit_received - reads clean_uspatentcitation and calculates citations received\n",
    "#     - cit_tree - reads clean_uspatentcitation and cit_made and calculates parent_citation\n",
    "#     - generalit - reads wipo and clean_uspatentcitation and calculates generality\n",
    "#     - originality - reads wipo and clean_uspatentcitation and calculates originality\n",
    "#     - wipo_first_class - reads wipo and generates wipo_first_class\n",
    "\n",
    "# it is now way faster, but I cannot know for sure if this is a consequence of coding or something changed in \n",
    "# infrastructure. \n",
    "\n",
    "# since class is a variable that also appear in non-cited patents, I'll include them in the final step\n",
    "\n",
    "\n",
    "# Jan, 20th 2020\n",
    "# join delay and tree data to form the analysis dataset\n",
    "\n",
    "\n",
    "# In[2]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from dask.delayed import delayed\n",
    "\n",
    "\n",
    "def data_read(file, names, usecols, dtype):\n",
    "    df = dd.read_parquet(file, compression='gzip', names=names, usecols=usecols, dtype=dtype, index_col='id', header=0)\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst='data/dataset.parquet.gz'\n",
    "\n",
    "patent='data/cleanpatent.parquet.gz'\n",
    "received='data/cit_received.parquet.gz'\n",
    "made='data/cit_made.parquet.gz'\n",
    "received_delay='data/cit_received_delay.parquet.gz'\n",
    "made_delay='data/cit_made_delay.parquet.gz'\n",
    "# cit_tree = 'data/cit_tree.csv.gz'\n",
    "originality = 'data/originality.parquet.gz'\n",
    "generality = 'data/generality.parquet.gz'\n",
    "wipo = 'data/wipo.parquet.gz'\n",
    "centrality='data/centrality_pagerank.csv'\n",
    "int_ext_cit='data/int_ext_cit.parquet.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs=[]\n",
    "\n",
    "#adopting patent.csv as the reference of patents\n",
    "df = dd.read_parquet(patent, compression='gzip', dtype=object)\n",
    "df['num_claims']=df['num_claims'].astype(float) #int does not handle NAN values, so using float instead\n",
    "# df=df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 2 entries, date to num_claims\n",
      "dtypes: float64(1), int64(1)"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outer join to citation received\n",
    "#there are patents who received citations that are not in patent.csv.gz\n",
    "#for example, the very ancient ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dd.read_parquet(received, compression='gzip')\n",
    "df=df.join(df2, how='outer')\n",
    "df['cit_received']=df['cit_received'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 3 entries, date to cit_received\n",
      "dtypes: float64(1), int64(2)"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i dont see any reason for patents making citations not to appear in patent.csv.gz\n",
    "#so from this data on, the list of patents is stable\n",
    "#the number of rows should not change (check)\n",
    "df2 = dd.read_parquet(made, compression='gzip')\n",
    "df=df.join(df2, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 4 entries, date to cit_made\n",
      "dtypes: float64(1), int64(3)"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dd.read_parquet(originality, compression='gzip')\n",
    "df=df.join(df2, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 5 entries, date to originality\n",
      "dtypes: float64(2), int64(3)"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dd.read_parquet(generality, compression='gzip')\n",
    "df=df.join(df2, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 6 entries, date to generality\n",
      "dtypes: float64(3), int64(3)"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dd.read_parquet(wipo, compression='gzip')\n",
    "df=df.join(df2, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = dd.read_parquet(int_ext_cit, compression='gzip', dtype=object)\n",
    "# df.set_index('index', inplace=True)\n",
    "df=df.join(df2, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 10 entries, date to far_external\n",
      "dtypes: category(2), float64(3), int64(5)"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dd.read_csv(centrality, dtype=object)\n",
    "df2=df2.set_index('id')\n",
    "df=df.join(df2, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding some new files\n",
    "inventor='data/inventor.parquet.gz'\n",
    "application='data/application.parquet.gz'\n",
    "foreign_cit='data/foreigncitation.parquet.gz'\n",
    "foreign_priority='data/foreign_priority.parquet.gz'\n",
    "pat_govt='data/df_pat_govt.parquet.gz'\n",
    "pat_assignee='data/pat_assignee.parquet.gz'\n",
    "otherref_cit = 'data/otherref_cit.parquet.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dd.read_parquet(inventor, dtype=object)\n",
    "df2=df2.set_index('id')\n",
    "df=df.join(df2, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dd.read_parquet(application)\n",
    "df2=df2.set_index('id')\n",
    "df=df.join(df2, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dd.read_parquet(foreign_cit)\n",
    "df2=df2.set_index('patent_id')\n",
    "df=df.join(df2, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dd.read_parquet(foreign_priority, dtype=object)\n",
    "df2=df2.set_index('patent_id')\n",
    "df=df.join(df2, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dd.read_parquet(pat_govt, dtype=object)\n",
    "df=df.join(df2, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dd.read_parquet(pat_assignee, dtype=object)\n",
    "#df2=df2.set_index('patent_id')\n",
    "df=df.join(df2, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dd.read_parquet(otherref_cit, dtype=object)\n",
    "#df2=df2.set_index('patent_id')\n",
    "df=df.join(df2, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=[np.object]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(dst, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[32]:\n",
    "\n",
    "\n",
    "# this code includes WIPO in the dataset\n",
    "# as class is also part of non-cited patents, I'm not including in here.\n",
    "# in any case, for some reason this merge could not be done by the method used in the previous\n",
    "# there is a bug when using read_csv.gz using dtype and indexing. \n",
    "# it is a known bug in the community but i do not know why the previous join did not accused the issue\n",
    "# the method below corrects the issue \n",
    "\n",
    "# names=['index', 'id', 'field_id']\n",
    "# dtype={'id':object}\n",
    "# usecols=['id', 'field_id']\n",
    "# # data_read(wipo, names, usecols, dtype)\n",
    "# wipo = pd.read_csv.gz(wipo, names=names, usecols=usecols, dtype=dtype, header=0)\n",
    "# wipo['id']=wipo['id'].astype(str)\n",
    "# wipo=wipo.set_index('id')\n",
    "# wipo.info()\n",
    "\n",
    "# dfs.append(wipo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
